{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis\n",
    "\n",
    "---\n",
    "\n",
    "Created by Jesper HÃ¸jmark Esbensen, 2018-12-01.\n",
    "\n",
    "This note book will create and train two agents to play tennis in the Unity Machine Learning environment \"Tennis\". The solution is based on the general deep reenforcement learning agent supplied in the cource [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "<img src=\"Tennis.png\",width=750>\n",
    "\n",
    "The environment consists of two \"Tennis\" rackets moved by an agent in the Unity environemnt. The rewards are setup to give +0.1 hitting the ball and -0.01 for dropping the ball on the floor. As both agents will try to maximizise thier rewards they will end up playing together for as long as possible.\n",
    "\n",
    "This and a number of other environments from Unity can be found here. [Unity environments](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher).\n",
    "\n",
    "\n",
    "### 1. Installation instructions\n",
    "\n",
    "Installation instructions can be found in the readme.md file in the github repository: https://github.com/JesperEsbensen/drlnd_collaboration_and_competition_project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. This solution\n",
    "\n",
    "The solution in this note book is heavely inspired by the solution to the continuous control project that can be found here: https://github.com/JesperEsbensen/drlnd-continuous-control. That was based on the bi-pedal environment given in the repository: https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal\n",
    "\n",
    "The solution here is modified by having multiple actor-critics network being trained in the tennis environment. Classes where added to handle the networks.\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "The implementation is based on the Deep Deterministic Policy Gradient algorithme described in the paper:\n",
    "\n",
    "[Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)\n",
    "\n",
    "The algorithm has shown to performe in different simulated environments. It is a continuous algorithm and therefore suitable for the tennis envrionment. \n",
    "The algorithm is based on the Deterministic Policy Gradient (DPG) method that is created around an actor-critic setup. To make the algorithm stable it uses a \"replay buffer\" and \"target/local network\". To add exploration it introduces noise to calculated actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The code \n",
    "\n",
    "#### Imports\n",
    "This cell handles the imports of the pyTorch and python modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Unity for the environment\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Import for file handling.\n",
    "import os\n",
    "\n",
    "# Import numpy for the math\n",
    "import numpy as np\n",
    "\n",
    "# Import torch for the AI\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# A little extra than standard python.\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# Time for timing\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "This cell defines a class for handling a Unity environment. It helps handle the start, close down and the extraction of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment ():\n",
    "    \n",
    "    # Encapsulates the environment and enables the use of 'with' to handle open/close.\n",
    "    \n",
    "    def __init__(self, envFile):\n",
    "        \"\"\"Initialize parameters for environment.\n",
    "        Params\n",
    "        ======\n",
    "            envFile (string): Path to envrionment executable\n",
    "        \"\"\"\n",
    "        self.filename = envFile\n",
    "        self.env = UnityEnvironment(file_name=envFile)\n",
    "            \n",
    "        # get the default brain\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "\n",
    "        # reset the environment\n",
    "        self.env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        \n",
    "        print ('Created unity enviroment from ', envFile)\n",
    "        self.dump_key_attributes()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "            \n",
    "    def state_size(self):\n",
    "        return self.env_info.vector_observations.shape[1]\n",
    "        \n",
    "    def action_size(self):\n",
    "        return self.brain.vector_action_space_size\n",
    "\n",
    "    def agent_size(self):\n",
    "        return len(self.env_info.agents)\n",
    "    \n",
    "    def reset(self, train_mode=True):\n",
    "        self.env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        return self.env_info.vector_observations, self.env_info.rewards, self.env_info.local_done\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.env_info = self.env.step(actions)[self.brain_name]\n",
    "        return self.env_info.vector_observations, self.env_info.rewards, self.env_info.local_done\n",
    "\n",
    "    #def states(self, train_mode=True):\n",
    "    #    return self.env.reset(train_mode)[self.brain_name]\n",
    "    \n",
    "    def dump_key_attributes (self):\n",
    "        print('Environment     :', self.filename)\n",
    "        print('Number of agents:', self.agent_size())\n",
    "        print('State size      :', self.state_size())\n",
    "        print('Action size     :', self.action_size())\n",
    "        \n",
    "    def end():\n",
    "        self.env.close()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility\n",
    "An utility function for initializing a layer in a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return -lim, lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardware\n",
    "\n",
    "Detect if a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor network\n",
    "\n",
    "The actor network to determine the actions to take from the given state. \n",
    "The network is build up from experiments. The structure is:\n",
    "\n",
    "* Fully connected layer from 2x24 input state points to 200 hidden nodes.\n",
    "* A batch normalization layer\n",
    "* A rectifying liniar activation functions (Relu) layer\n",
    "* A fully connected layer from the 200 hidden nodes to 100 hidden notes.\n",
    "* A batch normalization layer\n",
    "* A relu layer\n",
    "* A fully connected layer from the 100 hidden nodes to 2 activation outputs\n",
    "* A tanh activation function to create a action signal beween -1 and 1.\n",
    "\n",
    "after the actions is calculated a noise can be added to explore the state space. This is generated as a Ornstein-Uhlenbeck process, but reduced over time defined by a decay-rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, agent_size, seed, fc1_units=132, fc2_units=132):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size * agent_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        print (\"Actor net: Input \", state_size * agent_size, \"->\", fc1_units, \\\n",
    "               \"hidden layer \", fc1_units, \"->\", fc2_units, \\\n",
    "               \"output layer \", fc2_units, \"->\", action_size)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        f,t = hidden_init(self.fc1)\n",
    "        self.fc1.weight.data.uniform_(f,t)\n",
    "        f,t = hidden_init(self.fc2)\n",
    "        self.fc2.weight.data.uniform_(f,t)\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.fc3(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Actor ():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=300, fc2_units=150, device='cpu'):\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.local = ActorNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
    "        self.target = ActorNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
    "        self.optimizer = optim.Adam(self.local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        self.noise_gen = OUNoise(action_size, random_seed, theta=NOISE_THETA , sigma=NOISE_SIGMA)\n",
    "        #self.noise_gen = AltNoise(action_size, random_seed, level=0.5)        \n",
    "        self.noise_sizes = deque(maxlen=50)\n",
    "        self.action_sizes = deque(maxlen=50)\n",
    "        self.noise_decay = NOISE_DECAY\n",
    "        \n",
    "    def soft_update(self, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Î¸_target = Ï*Î¸_local + (1 - Ï)*Î¸_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(self.target.parameters(), self.local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "            \n",
    "    def act(self, state, add_noise ):\n",
    "        \n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        # Set network to evaluation mode\n",
    "        self.local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.local(state).cpu().data.numpy()\n",
    "\n",
    "        # Set network to training mode.\n",
    "        self.local.train()\n",
    "        \n",
    "        noise = 0.0\n",
    "        if add_noise:\n",
    "            noise = self.noise_gen.sample(action)\n",
    "            action += noise * self.noise_decay\n",
    "            \n",
    "        self.action_sizes.append(float(np.linalg.norm(action)))\n",
    "        self.noise_sizes.append(float(np.linalg.norm(noise)))\n",
    "            \n",
    "        # Make the noisy action look like the original action.\n",
    "        return action\n",
    "    \n",
    "    def new_episode (self):\n",
    "        self.noise_gen.reset()\n",
    "        self.noise_decay = self.noise_decay * NOISE_DECAY\n",
    "        \n",
    "    def noiserate(self):\n",
    "        self.noise_rate = np.mean(self.noise_sizes)/np.mean(self.action_sizes)       \n",
    "        return self.noise_rate\n",
    "    \n",
    "    def load (self, filename):\n",
    "        self.local.load_state_dict(torch.load(filename + '-actor-local.pth'))\n",
    "        self.target.load_state_dict(torch.load(filename + '-actor-target.pth'))\n",
    "        \n",
    "    def save (self, filename):\n",
    "        torch.save(self.local.state_dict(), filename + '-actor-local.pth')\n",
    "        torch.save(self.target.state_dict(), filename + '-actor-target.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic network\n",
    "\n",
    "The critic network to estimate the value of taking an action - the reward. \n",
    "The network is build up from experiments. The structure is:\n",
    "\n",
    "* Fully connected layer from 2x24 input state points to 300 hidden nodes.\n",
    "* A batch normalization layer\n",
    "* A rectifying liniar activation functions (Relu) layer\n",
    "* The output here is concatenated with the actions to evaluate. Into a 300 + 2*2 hidden layer.\n",
    "* A fully connected layer from 304 hidden notes to 150 hidden nodes.\n",
    "* A batch normalization layer\n",
    "* A relu layer\n",
    "* A fully connected layer from the 150 hidden nodes to 1 value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CriticNetwork (nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, agent_size, seed, fc1_units=300, fc2_units=150):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size * agent_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units + action_size * agent_size, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        print (\"Critic net: Input \", state_size * agent_size, \"->\", fc1_units, \\\n",
    "               \"hidden layer + actions\", fc1_units, \" + \", action_size * agent_size, \"->\", fc2_units, \\\n",
    "               \"hidden layer \", fc2_units, \"-> output \", 1)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        f,t = hidden_init(self.fc1)\n",
    "        self.fc1.weight.data.uniform_(f,t)\n",
    "        f,t = hidden_init(self.fc2)\n",
    "        self.fc2.weight.data.uniform_(f,t)\n",
    "        self.fc3.weight.data.uniform_(-3e-3,3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Critic ():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, agent_size, seed, fc1_units=100, fc2_units=50, device='cpu'):\n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        print (\"state size:\", state_size, \"action size:\", action_size, \"fc1:\", fc1_units, \"fc2_units:\", fc2_units)\n",
    "        self.local = CriticNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
    "        self.target = CriticNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
    "        self.optimizer = optim.Adam(self.local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "    def soft_update(self, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Î¸_target = Ï*Î¸_local + (1 - Ï)*Î¸_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(self.target.parameters(), self.local.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "    def load (self, filename):\n",
    "        self.local.load_state_dict(torch.load(filename + '-critic-local.pth'))\n",
    "        self.target.load_state_dict(torch.load(filename + '-critic-target.pth'))\n",
    "        \n",
    "    def save (self, filename):\n",
    "        torch.save(self.local.state_dict(), filename + '-critic-local.pth')\n",
    "        torch.save(self.target.state_dict(), filename + '-critic-target.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Agent\n",
    "\n",
    "The agent handles the networks: actor, critic and thier target and local versions. It handles the noice generator, the replay buffer, the act and learn steps.\n",
    "Note that a decay rate of the noice has been build in. This way the noise is higher in the beginning and becomes lower with the number of steps taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, index, environment, actor, critic, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.env = environment\n",
    "        \n",
    "        self.index = index\n",
    "        self.state_size = self.env.state_size()\n",
    "        self.action_size = self.env.action_size()\n",
    "        self.agent_size = self.env.agent_size()\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # Actor Network (w/ Target Network)   \n",
    "        self.actor = actor\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic = critic\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(self.action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        print ('Created agent: states = ', self.state_size, ' actions = ', self.action_size)\n",
    "\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        # Save experience / reward.\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        # (\"step\", state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE and self.step_counter % 1 == 0:\n",
    "            for _ in range(5):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "                \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        actions = self.actor.act(state, add_noise)\n",
    "        #print ('Actions:', actions, 'States:', state)\n",
    "        return actions  # np.clip(actions, -1, 1)\n",
    "    \n",
    "\n",
    "    def new_episode(self):\n",
    "        self.actor.new_episode()\n",
    "            \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + Î³ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor.target(next_states)\n",
    "        \n",
    "        if self.index == 0:\n",
    "            actions_next = torch.cat((actions_next, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            actions_next = torch.cat((actions[:,:2], actions_next), dim=1)\n",
    "\n",
    "        #actions_next = torch.cat((actions_next, actions_prev), dim=1)\n",
    "        Q_targets_next = self.critic.target(next_states, actions_next)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic.local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "            \n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor.local(states)\n",
    "        \n",
    "        if self.index == 0:\n",
    "            actions_pred = torch.cat((actions_pred, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            actions_pred = torch.cat((actions[:,:2], actions_pred), dim=1)\n",
    "\n",
    "        #actions_next = torch.cat((actions_pred, actions), dim=1)\n",
    "        actor_loss = -self.critic.local(states, actions_pred).mean()\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "            \n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.critic.soft_update(TAU)\n",
    "        self.actor.soft_update(TAU)     \n",
    "\n",
    "            \n",
    "    def environment(self):\n",
    "        return self.env\n",
    "           \n",
    "    def noise_rate(self):\n",
    "        return self.actor.noiserate()\n",
    "    \n",
    "    def load (self, filename):\n",
    "        self.actor.load(filename)\n",
    "        self.critic.load(filename)\n",
    "        \n",
    "    def save (self, filename):\n",
    "        self.actor.save(filename)\n",
    "        self.critic.save(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise\n",
    "\n",
    "Noise is introduced to the actions generated to enabled the network to explore \"alternative\" actions.\n",
    "Note that the Ornenstein-Uhlenbeck is modified to give negaive random numbers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.noise_decay = NOISE_DECAY\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        self.noise_decay = max (self.noise_decay * NOISE_DECAY, NOISE_MIN)\n",
    "\n",
    "    def sample(self, action):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        #dx = self.theta * (self.mu - x) + self.sigma * np.array([(random.random() - 0.5)*2 for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "\n",
    "        return self.state * self.noise_decay\n",
    "    \n",
    "class AltNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, level=0.1):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.seed = random.seed(seed)\n",
    "        self.noise_decay = NOISE_DECAY\n",
    "        self.level = level\n",
    "        \n",
    "    def sample(self, action):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        al = np.linalg.norm(action)\n",
    "        no = np.array([(random.random() - 0.5)*2*self.level*al for i in range(len(action))])\n",
    "        \n",
    "        return no * self.noise_decay\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise_decay = max (self.noise_decay * NOISE_DECAY, NOISE_MIN)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffer\n",
    "\n",
    "The replay buffer is a buffer where tuples of stats, action, rewards are stored. They can in random order be sampled and used  in the learning process. By selecting the steps the correlation between steps is broken.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, reward_level=-1.0):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): action size\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (): For random generation and initilization of the noise generator.\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # Internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.rf = 0\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # We add all actions that gave a success, and only some that dident.\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop\n",
    "\n",
    "The main loop drives the process of having the agent act in the environment.\n",
    "Loop episodes and timesteps to drive the environment, agent and learning.\n",
    "\n",
    "When the goal is reached the process will continue for another 25% episodes to see what happens. A loop will start from a set of actor/critics files if they exist. A set of actor and critics network is stored every time a better result is reached and a separate set when the goal is reached.\n",
    "\n",
    "In addition to the average score this the main loop will display:\n",
    "* reward fraction - number of actions with success of all of the stored actions (for the experiment)\n",
    "* Noice level - vector length of noise added compared to action vector.\n",
    "* Time - loop time of episode.\n",
    "\n",
    "For every episode, if a better result has been achieved, the parameters are stored. When the goal is reached the parameters are also stored - and kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ddpg(name, environment, agents, n_episodes=2000, max_t=10000, print_every=100, goal=0.5, reuseNetwork=False, mode='train'):\n",
    "    \n",
    "    checkpointfile = name + '-agent1-best-actor-local.pth'\n",
    "    if reuseNetwork:\n",
    "        if os.path.isfile(checkpointfile):\n",
    "            print('\\nLoading checkpoints:', checkpointfile)\n",
    "            agents[0].load(name + '-agent1-best')\n",
    "            agents[1].load(name + '-agent2-best')\n",
    "        else:\n",
    "            print('\\nRestart learning - no checkpoint file found:', checkpointfile)\n",
    "    else:\n",
    "        print('\\nStarting with a freshly initialized network.')        \n",
    "        \n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    scores_sliding = []\n",
    "    best_score = 0\n",
    "    goal_passed = 0\n",
    "    goal_passed_counter = 0\n",
    "    \n",
    "    # Drive episodes.\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        states, rewards, dones = environment.reset()\n",
    "        state = np.reshape(states, (1, env.state_size() * len(agents)))\n",
    "        \n",
    "        score_agents = np.zeros(env.agent_size())\n",
    "        \n",
    "        agents[0].new_episode()\n",
    "        agents[1].new_episode()\n",
    "\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            # Calculate best actions for this step. \n",
    "            action1 = agents[0].act( state, mode == 'train' )\n",
    "            action2 = agents[1].act( state, mode == 'train' )\n",
    "\n",
    "            actions = np.concatenate((action1, action2), axis=0) \n",
    "            actions = np.reshape(actions, (1, env.action_size() * len(agents)))\n",
    "            \n",
    "            # Determin next state.\n",
    "            next_states, rewards, dones = environment.step( actions )  # send the action to the environment\n",
    "            next_state = np.reshape(next_states, (1, env.state_size() * len(agents)))\n",
    "            \n",
    "            # Agent takes step and learns.\n",
    "            if mode == 'train':\n",
    "                agents[0].step(state, actions, rewards[0], next_state, dones[0])\n",
    "                agents[1].step(state, actions, rewards[1], next_state, dones[1])\n",
    "                \n",
    "            # Prepare next loop.\n",
    "            states = next_states\n",
    "            state = next_state\n",
    "            score_agents += rewards\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "                \n",
    "            if not mode == 'train':\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        score = np.max(score_agents)\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        score_average = np.mean(scores_deque)\n",
    "        scores_sliding.append(score_average)\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        # Only print results if we are training.\n",
    "        if mode == 'train':\n",
    "            # Store model parameters if it is better than the last best.\n",
    "            if score_average > best_score:\n",
    "                best_score = score_average\n",
    "                agents[0].save( name + '-agent1-best')\n",
    "                agents[1].save( name + '-agent2-best')\n",
    "\n",
    "                print('\\rEpisode {}, Average Score: {:.3f}, ({:.3f}/{:.3f}), Noise rate {:.3f}, Time {:.2f}. Better model.'.format( \\\n",
    "                    i_episode, np.mean(scores_deque), rewards[0], rewards[1], agents[0].noise_rate(), \\\n",
    "                    end - start))\n",
    "            else:\n",
    "                print('\\rEpisode {}, Average Score: {:.6f}, ({:.3f}/{:.3f}), Noise rate {:.3f}, Time {:.2f}.'.format( \\\n",
    "                    i_episode, np.mean(scores_deque), rewards[0], rewards[1], agents[0].noise_rate(), \\\n",
    "                    end - start), end=\"\" )\n",
    "\n",
    "            if i_episode % print_every == 0:\n",
    "                print('\\rEpisode {}, Average Score: {:.3f}, ({:.3f}/{:.3f}), Noise rate {:.3f}, Time {:.2f}.'.format( \\\n",
    "                    i_episode, np.mean(scores_deque), rewards[0], rewards[1], agents[0].noise_rate(), \\\n",
    "                    end - start))\n",
    "                print (\"Actions \", action1, action2)\n",
    "\n",
    "            if score_average >= goal:\n",
    "                if goal_passed == 0:\n",
    "                    goal_passed = i_episode\n",
    "\n",
    "                    print('\\nGoal reached in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, score_average))\n",
    "                    agents[0].save( name + '-agent1-goal')\n",
    "                    agents[1].save( name + '-agent2-goal')\n",
    "\n",
    "            if goal_passed > 0:\n",
    "                goal_passed_counter = goal_passed_counter + 1\n",
    "\n",
    "            if goal_passed > 0 and goal_passed_counter > goal_passed * 0.2: # Run addtional 20% episodes to see what happens.\n",
    "                break\n",
    "        else:\n",
    "            # Wait for 3 seconds before playing next episode\n",
    "            time.sleep(3)\n",
    "            \n",
    "    return scores, scores_sliding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "\n",
    "Below you find two functions for plotting the scores collected. For one timeseries and for comparing two timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotScores (scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCompareScores (score1, score2):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(score1)+1), score1)\n",
    "    plt.plot(np.arange(1, len(score2)+1), score2)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execution\n",
    "\n",
    "#### Test the reward level experiment\n",
    "The two networks are executet and compared to see what effect the reward level experiment has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created unity enviroment from  .\\Tennis_Windows_x86_64\\Tennis_Windows_x86_64\\Tennis.exe\n",
      "Environment     : .\\Tennis_Windows_x86_64\\Tennis_Windows_x86_64\\Tennis.exe\n",
      "Number of agents: 2\n",
      "State size      : 24\n",
      "Action size     : 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "state size: 24 action size: 2 fc1: 100 fc2_units: 50\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "state size: 24 action size: 2 fc1: 100 fc2_units: 50\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "Created agent: states =  24  actions =  2\n",
      "Created agent: states =  24  actions =  2\n",
      "\n",
      "Starting with a freshly initialized network.\n",
      "Episode 1, Average Score: 0.100, (-0.010/0.000), Noise rate 1.026, Time 0.84. Better model.\n",
      "Episode 100, Average Score: 0.019, (0.000/-0.010), Noise rate 0.511, Time 2.60.60.\n",
      "Actions  [[-0.93776083  0.91994274]] [[ 0.13260049 -1.148736  ]]\n",
      "Episode 200, Average Score: 0.019, (-0.010/0.000), Noise rate 0.147, Time 2.70.70.\n",
      "Actions  [[1.0164471  0.20869432]] [[-0.889805  -0.9973145]]\n",
      "Episode 300, Average Score: 0.038, (0.000/-0.010), Noise rate 0.052, Time 2.87.87.\n",
      "Actions  [[-1.000355   0.9624961]] [[-0.8102112 -1.0002573]]\n",
      "Episode 400, Average Score: 0.036, (-0.010/0.000), Noise rate 0.017, Time 3.31.31..\n",
      "Actions  [[-0.00471437 -0.8184519 ]] [[0.9999546  0.38079292]]\n",
      "Episode 500, Average Score: 0.041, (0.000/-0.010), Noise rate 0.007, Time 5.78.78..\n",
      "Actions  [[0.9999627 0.9232883]] [[ 0.98989993 -0.98773116]]\n",
      "Episode 600, Average Score: 0.069, (0.000/-0.010), Noise rate 0.002, Time 5.70.70..\n",
      "Actions  [[-0.9682591  0.9832555]] [[0.44878697 0.99998677]]\n",
      "Episode 700, Average Score: 0.068, (-0.010/0.000), Noise rate 0.001, Time 5.81.81..\n",
      "Actions  [[-0.99931407  0.9996099 ]] [[-0.9999996 -1.0000011]]\n",
      "Episode 800, Average Score: 0.077, (-0.010/0.000), Noise rate 0.000, Time 2.47.47..\n",
      "Actions  [[ 0.999976  -1.0000001]] [[-0.99999994 -0.9999985 ]]\n",
      "Episode 898, Average Score: 0.101, (0.000/-0.010), Noise rate 0.000, Time 21.64. Better model.\n",
      "Episode 900, Average Score: 0.103, (0.000/-0.010), Noise rate 0.000, Time 14.00. Better model.\n",
      "Episode 900, Average Score: 0.103, (0.000/-0.010), Noise rate 0.000, Time 14.00.\n",
      "Actions  [[ 0.9818072 -0.9297911]] [[0.9999807 1.       ]]\n",
      "Episode 903, Average Score: 0.105, (0.000/-0.010), Noise rate 0.000, Time 29.30. Better model.\n",
      "Episode 904, Average Score: 0.106, (0.000/-0.010), Noise rate 0.000, Time 6.04. Better model.\n",
      "Episode 906, Average Score: 0.108, (0.000/-0.010), Noise rate 0.000, Time 22.43. Better model.\n",
      "Episode 907, Average Score: 0.110, (0.000/-0.010), Noise rate 0.000, Time 26.36. Better model.\n",
      "Episode 911, Average Score: 0.110, (0.000/-0.010), Noise rate 0.000, Time 6.00. Better model.\n",
      "Episode 912, Average Score: 0.110, (0.000/-0.010), Noise rate 0.000, Time 6.32. Better model.\n",
      "Episode 914, Average Score: 0.111, (0.000/-0.010), Noise rate 0.000, Time 13.22. Better model.\n",
      "Episode 915, Average Score: 0.113, (-0.010/0.000), Noise rate 0.000, Time 25.60. Better model.\n",
      "Episode 917, Average Score: 0.114, (-0.010/0.000), Noise rate 0.000, Time 13.95. Better model.\n",
      "Episode 928, Average Score: 0.114, (0.000/-0.010), Noise rate 0.000, Time 6.06. Better model.\n",
      "Episode 930, Average Score: 0.115, (0.000/-0.010), Noise rate 0.000, Time 6.30. Better model.\n",
      "Episode 933, Average Score: 0.116, (-0.010/0.000), Noise rate 0.000, Time 5.90. Better model.\n",
      "Episode 934, Average Score: 0.117, (0.000/-0.010), Noise rate 0.000, Time 13.62. Better model.\n",
      "Episode 936, Average Score: 0.120, (0.000/-0.010), Noise rate 0.000, Time 21.29. Better model.\n",
      "Episode 939, Average Score: 0.121, (0.000/-0.010), Noise rate 0.000, Time 13.88. Better model.\n",
      "Episode 940, Average Score: 0.121, (0.000/-0.010), Noise rate 0.000, Time 6.34. Better model.\n",
      "Episode 941, Average Score: 0.122, (0.000/-0.010), Noise rate 0.000, Time 13.99. Better model.\n",
      "Episode 953, Average Score: 0.123, (0.000/-0.010), Noise rate 0.000, Time 14.23. Better model.\n",
      "Episode 954, Average Score: 0.124, (0.000/-0.010), Noise rate 0.000, Time 18.00. Better model.\n",
      "Episode 955, Average Score: 0.125, (0.000/-0.010), Noise rate 0.000, Time 13.87. Better model.\n",
      "Episode 959, Average Score: 0.126, (0.000/-0.010), Noise rate 0.000, Time 7.15. Better model.\n",
      "Episode 960, Average Score: 0.127, (-0.010/0.000), Noise rate 0.000, Time 10.16. Better model.\n",
      "Episode 961, Average Score: 0.128, (0.000/-0.010), Noise rate 0.000, Time 13.48. Better model.\n",
      "Episode 962, Average Score: 0.130, (-0.010/0.000), Noise rate 0.000, Time 25.04. Better model.\n",
      "Episode 963, Average Score: 0.131, (0.000/-0.010), Noise rate 0.000, Time 10.18. Better model.\n",
      "Episode 964, Average Score: 0.133, (0.000/-0.010), Noise rate 0.000, Time 21.35. Better model.\n",
      "Episode 967, Average Score: 0.134, (0.000/-0.010), Noise rate 0.000, Time 17.93. Better model.\n",
      "Episode 968, Average Score: 0.135, (0.000/-0.010), Noise rate 0.000, Time 5.94. Better model.\n",
      "Episode 969, Average Score: 0.136, (-0.010/0.000), Noise rate 0.000, Time 13.98. Better model.\n",
      "Episode 971, Average Score: 0.137, (0.000/-0.010), Noise rate 0.000, Time 18.51. Better model.\n",
      "Episode 972, Average Score: 0.139, (0.000/-0.010), Noise rate 0.000, Time 21.65. Better model.\n",
      "Episode 973, Average Score: 0.141, (-0.010/0.000), Noise rate 0.000, Time 17.63. Better model.\n",
      "Episode 974, Average Score: 0.142, (-0.010/0.000), Noise rate 0.000, Time 14.12. Better model.\n",
      "Episode 987, Average Score: 0.143, (0.000/-0.010), Noise rate 0.000, Time 14.14. Better model.\n",
      "Episode 988, Average Score: 0.147, (0.000/-0.010), Noise rate 0.000, Time 43.72. Better model.\n",
      "Episode 990, Average Score: 0.148, (-0.010/0.000), Noise rate 0.000, Time 17.73. Better model.\n",
      "Episode 994, Average Score: 0.149, (-0.010/0.000), Noise rate 0.000, Time 17.58. Better model.\n",
      "Episode 996, Average Score: 0.150, (0.000/-0.010), Noise rate 0.000, Time 38.18. Better model.\n",
      "Episode 999, Average Score: 0.154, (0.000/-0.010), Noise rate 0.000, Time 54.97. Better model.\n",
      "Episode 1000, Average Score: 0.152, (-0.010/0.000), Noise rate 0.000, Time 2.75.75.\n",
      "Actions  [[-1. -1.]] [[-1.        -0.9451831]]\n",
      "Episode 1029, Average Score: 0.155, (0.000/-0.010), Noise rate 0.000, Time 18.25. Better model.\n",
      "Episode 1031, Average Score: 0.156, (0.000/-0.010), Noise rate 0.000, Time 13.75. Better model.\n",
      "Episode 1034, Average Score: 0.158, (0.000/-0.010), Noise rate 0.000, Time 28.80. Better model.\n",
      "Episode 1035, Average Score: 0.159, (0.000/-0.010), Noise rate 0.000, Time 13.81. Better model.\n",
      "Episode 1037, Average Score: 0.159, (0.000/-0.010), Noise rate 0.000, Time 16.59. Better model.\n",
      "Episode 1038, Average Score: 0.162, (-0.010/0.000), Noise rate 0.000, Time 29.98. Better model.\n",
      "Episode 1043, Average Score: 0.164, (0.000/-0.010), Noise rate 0.000, Time 37.88. Better model.\n",
      "Episode 1044, Average Score: 0.164, (0.000/-0.010), Noise rate 0.000, Time 20.37. Better model.\n",
      "Episode 1045, Average Score: 0.170, (0.000/-0.010), Noise rate 0.000, Time 59.15. Better model.\n",
      "Episode 1046, Average Score: 0.171, (0.000/-0.010), Noise rate 0.000, Time 13.48. Better model.\n",
      "Episode 1053, Average Score: 0.180, (0.000/-0.010), Noise rate 0.000, Time 93.53. Better model.\n",
      "Episode 1054, Average Score: 0.181, (0.000/-0.010), Noise rate 0.000, Time 28.25. Better model.\n",
      "Episode 1055, Average Score: 0.185, (-0.010/0.000), Noise rate 0.000, Time 46.01. Better model.\n",
      "Episode 1056, Average Score: 0.186, (-0.010/0.000), Noise rate 0.000, Time 15.03. Better model.\n",
      "Episode 1058, Average Score: 0.187, (-0.010/0.000), Noise rate 0.000, Time 17.97. Better model.\n",
      "Episode 1059, Average Score: 0.191, (0.000/-0.010), Noise rate 0.000, Time 41.07. Better model.\n",
      "Episode 1064, Average Score: 0.193, (0.000/-0.010), Noise rate 0.000, Time 55.76. Better model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1065, Average Score: 0.206, (0.000/-0.010), Noise rate 0.000, Time 117.21. Better model.\n",
      "Episode 1066, Average Score: 0.208, (0.000/-0.010), Noise rate 0.000, Time 17.98. Better model.\n",
      "Episode 1067, Average Score: 0.210, (-0.010/0.000), Noise rate 0.000, Time 28.73. Better model.\n",
      "Episode 1068, Average Score: 0.212, (0.000/-0.010), Noise rate 0.000, Time 24.73. Better model.\n",
      "Episode 1069, Average Score: 0.214, (-0.010/0.000), Noise rate 0.000, Time 32.70. Better model.\n",
      "Episode 1070, Average Score: 0.216, (0.000/-0.010), Noise rate 0.000, Time 26.13. Better model.\n",
      "Episode 1074, Average Score: 0.220, (0.000/-0.010), Noise rate 0.000, Time 62.28. Better model.\n",
      "Episode 1075, Average Score: 0.222, (-0.010/0.000), Noise rate 0.000, Time 13.45. Better model.\n",
      "Episode 1076, Average Score: 0.224, (0.000/-0.010), Noise rate 0.000, Time 21.38. Better model.\n",
      "Episode 1077, Average Score: 0.225, (-0.010/0.000), Noise rate 0.000, Time 20.80. Better model.\n",
      "Episode 1079, Average Score: 0.233, (-0.010/0.000), Noise rate 0.000, Time 62.56. Better model.\n",
      "Episode 1080, Average Score: 0.258, (0.000/0.000), Noise rate 0.000, Time 197.17. Better model.\n",
      "Episode 1082, Average Score: 0.260, (-0.010/0.000), Noise rate 0.000, Time 13.50. Better model.\n",
      "Episode 1083, Average Score: 0.267, (0.000/-0.010), Noise rate 0.000, Time 89.25. Better model.\n",
      "Episode 1084, Average Score: 0.270, (-0.010/0.000), Noise rate 0.000, Time 25.61. Better model.\n",
      "Episode 1086, Average Score: 0.273, (-0.010/0.000), Noise rate 0.000, Time 75.35. Better model.\n",
      "Episode 1087, Average Score: 0.279, (-0.010/0.000), Noise rate 0.000, Time 63.92. Better model.\n",
      "Episode 1089, Average Score: 0.283, (0.000/-0.010), Noise rate 0.000, Time 52.35. Better model.\n",
      "Episode 1090, Average Score: 0.286, (-0.010/0.000), Noise rate 0.000, Time 40.01. Better model.\n",
      "Episode 1091, Average Score: 0.296, (0.000/-0.010), Noise rate 0.000, Time 85.43. Better model.\n",
      "Episode 1092, Average Score: 0.299, (0.000/-0.010), Noise rate 0.000, Time 30.41. Better model.\n",
      "Episode 1093, Average Score: 0.311, (0.000/-0.010), Noise rate 0.000, Time 106.57. Better model.\n",
      "Episode 1096, Average Score: 0.326, (0.000/-0.010), Noise rate 0.000, Time 158.56. Better model.\n",
      "Episode 1097, Average Score: 0.332, (-0.010/0.000), Noise rate 0.000, Time 57.73. Better model.\n",
      "Episode 1098, Average Score: 0.337, (0.000/-0.010), Noise rate 0.000, Time 64.64. Better model.\n",
      "Episode 1099, Average Score: 0.339, (-0.010/0.000), Noise rate 0.000, Time 74.34. Better model.\n",
      "Episode 1100, Average Score: 0.341, (-0.010/0.000), Noise rate 0.000, Time 14.10. Better model.\n",
      "Episode 1100, Average Score: 0.341, (-0.010/0.000), Noise rate 0.000, Time 14.10.\n",
      "Actions  [[-0.58044016  1.        ]] [[-1.          0.99976397]]\n",
      "Episode 1101, Average Score: 0.343, (-0.010/0.000), Noise rate 0.000, Time 25.76. Better model.\n",
      "Episode 1102, Average Score: 0.346, (-0.010/0.000), Noise rate 0.000, Time 22.35. Better model.\n",
      "Episode 1103, Average Score: 0.350, (-0.010/0.000), Noise rate 0.000, Time 42.41. Better model.\n",
      "Episode 1105, Average Score: 0.353, (-0.010/0.000), Noise rate 0.000, Time 52.75. Better model.\n",
      "Episode 1107, Average Score: 0.356, (0.000/-0.010), Noise rate 0.000, Time 31.97. Better model.\n",
      "Episode 1108, Average Score: 0.357, (-0.010/0.000), Noise rate 0.000, Time 17.63. Better model.\n",
      "Episode 1109, Average Score: 0.357, (0.000/-0.010), Noise rate 0.000, Time 28.29. Better model.\n",
      "Episode 1111, Average Score: 0.360, (-0.010/0.000), Noise rate 0.000, Time 44.36. Better model.\n",
      "Episode 1112, Average Score: 0.360, (0.000/-0.010), Noise rate 0.000, Time 9.87. Better model.\n",
      "Episode 1116, Average Score: 0.371, (0.000/-0.010), Noise rate 0.000, Time 96.16. Better model.\n",
      "Episode 1117, Average Score: 0.380, (-0.010/0.000), Noise rate 0.000, Time 72.29. Better model.\n",
      "Episode 1118, Average Score: 0.382, (0.000/-0.010), Noise rate 0.000, Time 20.83. Better model.\n",
      "Episode 1119, Average Score: 0.383, (0.000/-0.010), Noise rate 0.000, Time 13.98. Better model.\n",
      "Episode 1120, Average Score: 0.387, (-0.010/0.000), Noise rate 0.000, Time 26.59. Better model.\n",
      "Episode 1121, Average Score: 0.393, (-0.010/0.000), Noise rate 0.000, Time 49.95. Better model.\n",
      "Episode 1122, Average Score: 0.395, (-0.010/0.000), Noise rate 0.000, Time 26.70. Better model.\n",
      "Episode 1126, Average Score: 0.398, (0.000/-0.010), Noise rate 0.000, Time 36.38. Better model.\n",
      "Episode 1128, Average Score: 0.399, (0.000/-0.010), Noise rate 0.000, Time 16.94. Better model.\n",
      "Episode 1129, Average Score: 0.402, (0.000/-0.010), Noise rate 0.000, Time 40.56. Better model.\n",
      "Episode 1132, Average Score: 0.405, (0.000/-0.010), Noise rate 0.000, Time 49.83. Better model.\n",
      "Episode 1133, Average Score: 0.406, (0.000/-0.010), Noise rate 0.000, Time 13.71. Better model.\n",
      "Episode 1135, Average Score: 0.417, (0.000/-0.010), Noise rate 0.000, Time 99.27. Better model.\n",
      "Episode 1136, Average Score: 0.418, (-0.010/0.000), Noise rate 0.000, Time 15.11. Better model.\n",
      "Episode 1137, Average Score: 0.422, (-0.010/0.000), Noise rate 0.000, Time 44.18. Better model.\n",
      "Episode 1141, Average Score: 0.429, (-0.010/0.000), Noise rate 0.000, Time 62.16. Better model.\n",
      "Episode 1142, Average Score: 0.432, (-0.010/0.000), Noise rate 0.000, Time 27.53. Better model.\n",
      "Episode 1143, Average Score: 0.433, (-0.010/0.000), Noise rate 0.000, Time 44.31. Better model.\n",
      "Episode 1146, Average Score: 0.438, (-0.010/0.000), Noise rate 0.000, Time 91.70. Better model.\n",
      "Episode 1147, Average Score: 0.459, (-0.010/0.000), Noise rate 0.000, Time 170.07. Better model.\n",
      "Episode 1148, Average Score: 0.460, (0.000/-0.010), Noise rate 0.000, Time 11.81. Better model.\n",
      "Episode 1149, Average Score: 0.469, (-0.010/0.000), Noise rate 0.000, Time 64.87. Better model.\n",
      "Episode 1150, Average Score: 0.482, (0.000/-0.010), Noise rate 0.000, Time 108.69. Better model.\n",
      "Episode 1151, Average Score: 0.493, (-0.010/0.000), Noise rate 0.000, Time 93.44. Better model.\n",
      "Episode 1152, Average Score: 0.502, (0.090/0.000), Noise rate 0.000, Time 72.66. Better model.\n",
      "\n",
      "Goal reached in 1152 episodes!\tAverage Score: 0.502\n",
      "Episode 1163, Average Score: 0.503, (0.000/-0.010), Noise rate 0.000, Time 50.57. Better model.\n",
      "Episode 1200, Average Score: 0.472, (0.000/-0.010), Noise rate 0.000, Time 28.54.54..\n",
      "Actions  [[1.         0.99999934]] [[-1. -1.]]\n",
      "Episode 1210, Average Score: 0.505, (-0.010/0.000), Noise rate 0.000, Time 136.35. Better model.\n",
      "Episode 1213, Average Score: 0.507, (0.000/-0.010), Noise rate 0.000, Time 23.66. Better model.\n",
      "Episode 1214, Average Score: 0.513, (0.000/-0.010), Noise rate 0.000, Time 61.70. Better model.\n",
      "Episode 1215, Average Score: 0.514, (0.000/-0.010), Noise rate 0.000, Time 15.62. Better model.\n",
      "Episode 1219, Average Score: 0.526, (0.000/0.000), Noise rate 0.000, Time 192.67. Better model.\n",
      "Episode 1222, Average Score: 0.529, (-0.010/0.000), Noise rate 0.000, Time 67.58. Better model.\n",
      "Episode 1225, Average Score: 0.542, (-0.010/0.000), Noise rate 0.000, Time 116.57. Better model.\n",
      "Episode 1226, Average Score: 0.543, (0.000/-0.010), Noise rate 0.000, Time 43.50. Better model.\n",
      "Episode 1227, Average Score: 0.544, (0.000/-0.010), Noise rate 0.000, Time 9.72. Better model.\n",
      "Episode 1230, Average Score: 0.547, (0.000/-0.010), Noise rate 0.000, Time 19.53. Better model.\n",
      "Episode 1231, Average Score: 0.555, (0.000/-0.010), Noise rate 0.000, Time 57.76. Better model.\n",
      "Episode 1234, Average Score: 0.571, (0.000/-0.010), Noise rate 0.000, Time 187.30. Better model.\n",
      "Episode 1239, Average Score: 0.576, (-0.010/0.000), Noise rate 0.000, Time 86.72. Better model.\n",
      "Episode 1240, Average Score: 0.584, (0.000/-0.010), Noise rate 0.000, Time 57.25. Better model.\n",
      "Episode 1244, Average Score: 0.592, (0.000/-0.010), Noise rate 0.000, Time 120.97. Better model.\n",
      "Episode 1245, Average Score: 0.594, (0.000/-0.010), Noise rate 0.000, Time 18.53. Better model.\n",
      "Episode 1300, Average Score: 0.385, (0.000/-0.010), Noise rate 0.000, Time 5.72.72..\n",
      "Actions  [[ 1. -1.]] [[-0.99981606 -1.        ]]\n",
      "Episode 1382, Average Score: 0.196800, (0.000/-0.010), Noise rate 0.000, Time 47.21.."
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 512        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 7.5e-2            # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.00     # L2 weight decay\n",
    "NOISE_SIGMA = 1.00      # Ornstein-Uhlenbeck parameter\n",
    "NOISE_THETA = 1.00      # Ornstein-Uhlenbeck parameter\n",
    "NOISE_DECAY = 0.99      # Decay rate per episode for the noise\n",
    "NOISE_MIN   = 0.0001    # Ensure a minumum noise.\n",
    "\n",
    "with Environment (envFile='.\\Tennis_Windows_x86_64\\Tennis_Windows_x86_64\\Tennis.exe') as env:    \n",
    "    \n",
    "    random_seed = 2\n",
    "    \n",
    "    # Build the agent\n",
    "    actor1 = Actor(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)\n",
    "    actor2 = Actor(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)  \n",
    "    \n",
    "    critic1 = Critic(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)\n",
    "    critic2 = Critic(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)\n",
    "    \n",
    "    agents = [\n",
    "        Agent( 0, env, actor1, critic1, random_seed=2 ),\n",
    "        Agent( 1, env, actor2, critic2, random_seed=2 )\n",
    "    ]\n",
    "        \n",
    "    # Train the agent.\n",
    "    scores, sliding = ddpg ( 'Tennis', env, agents, goal=0.5, n_episodes=2000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqotoikMcsdEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotCompareScores (scores, sliding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal was reached in 1152  episodes, and it reached almost 0.6 in before breaking down. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play a game\n",
    "\n",
    "Take the best network play a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created unity enviroment from  .\\Tennis_Windows_x86_64\\Tennis_Windows_x86_64\\Tennis.exe\n",
      "Environment     : .\\Tennis_Windows_x86_64\\Tennis_Windows_x86_64\\Tennis.exe\n",
      "Number of agents: 2\n",
      "State size      : 24\n",
      "Action size     : 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "Actor net: Input  48 -> 150 hidden layer  150 -> 132 output layer  132 -> 2\n",
      "state size: 24 action size: 2 fc1: 100 fc2_units: 50\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "state size: 24 action size: 2 fc1: 100 fc2_units: 50\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "Critic net: Input  48 -> 50 hidden layer + actions 50  +  4 -> 150 hidden layer  150 -> output  1\n",
      "Created agent: states =  24  actions =  2\n",
      "Created agent: states =  24  actions =  2\n",
      "\n",
      "Loading checkpoints: Tennis-agent1-best-actor-local.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with Environment (envFile='.\\Tennis_Windows_x86_64\\Tennis_Windows_x86_64\\Tennis.exe') as env:    \n",
    "    \n",
    "    random_seed = 2\n",
    "    \n",
    "    # Build the agent\n",
    "    actor1 = Actor(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)\n",
    "    actor2 = Actor(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)  \n",
    "    \n",
    "    critic1 = Critic(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)\n",
    "    critic2 = Critic(env.state_size(), env.action_size(), env.agent_size(), random_seed, device=device)\n",
    "    \n",
    "    agents = [\n",
    "        Agent( 0, env, actor1, critic1, random_seed=2 ),\n",
    "        Agent( 1, env, actor2, critic2, random_seed=2 )\n",
    "    ]\n",
    "        \n",
    "    # Train the agent.\n",
    "    scores, sliding = ddpg ( 'Tennis', env, agents, goal=0.5, n_episodes=5, reuseNetwork=True, mode='play'  )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conclusion\n",
    "\n",
    "It is possible to create a deep reinforcement learning network that can play tennis in the Unity environment. It is very important to be very precise in the design and programming of the network. Small errors are sometimes compensated partly by the flexibility of the network and there fore difficult to identify as errors. Also designs and choice of hyper parameters that seem intuitive have the opposite influence than expected. \n",
    "\n",
    "It is important to have a focus on timing the algorithems. The learning cycle for the developper is increased greatly by the long learning sessions that are needed by the network.\n",
    "\n",
    "Printing out state, actions, noise levels, etc is and important way to get visibility into the calculations done and identify a breakdown or an error in the system.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Future improvements\n",
    "\n",
    "Future improvements could be:\n",
    "* Automation of hyper parameter finding. Ex: grid search.\n",
    "* Futher fine tuning of the hyper parameters. \n",
    "* Try out other deep reinforcement learning algorithmes like PPO\n",
    "* Investigate why the network seems to unlearn and degenerate completely at some point.\n",
    "* It would be interesting to see how different designs for critic (centrelized or individual) would influence the learning.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
